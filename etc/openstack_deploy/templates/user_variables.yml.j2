# {{ ansible_managed }}
---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###
### This file contains commonly used overrides for convenience. Please inspect
### the defaults for each role to find additional override options.
###

## Debug and Verbose options (pay attention to high log verbosity when debug set to true).
debug: false

{% if low_end_profile is defined and low_end_profile | bool %}
## Galera settings
galera_innodb_buffer_pool_size: 16M
galera_innodb_log_buffer_size: 4M
galera_wsrep_provider_options:
 - { option: "gcache.size", value: "4M" }

## Neutron settings
neutron_metadata_checksum_fix: True

#### Set workers for all services to optimise memory usage

### Repo
repo_nginx_threads: 2

### Keystone
keystone_httpd_mpm_start_servers: 2
keystone_httpd_mpm_min_spare_threads: 1
keystone_httpd_mpm_max_spare_threads: 2
keystone_httpd_mpm_thread_limit: 2
keystone_httpd_mpm_thread_child: 1
keystone_wsgi_threads: 1
keystone_wsgi_processes_max: 2

### Cinder
cinder_wsgi_processes_max: 2
cinder_wsgi_threads: 1
cinder_wsgi_buffer_size: 16384
cinder_osapi_volume_workers_max: 2

## Glance
glance_api_threads_max: 2
glance_api_threads: 1
glance_api_workers: 1
glance_registry_workers: 1
glance_wsgi_threads: 1
glance_wsgi_processes_max: 2
glance_wsgi_processes: 2

## Nova
nova_wsgi_threads: 1
nova_wsgi_processes_max: 2
nova_wsgi_processes: 2
nova_wsgi_buffer_size: 16384
nova_api_threads_max: 2
nova_api_threads: 1
nova_osapi_compute_workers: 1
nova_conductor_workers: 1
nova_metadata_workers: 1

## Neutron
neutron_rpc_workers: 1
neutron_metadata_workers: 1
neutron_api_workers: 1
neutron_api_threads_max: 2
neutron_api_threads: 2
neutron_num_sync_threads: 1

## Heat
heat_api_workers: 1
heat_api_threads_max: 2
heat_api_threads: 1
heat_wsgi_threads: 1
heat_wsgi_processes_max: 2
heat_wsgi_processes: 1
heat_wsgi_buffer_size: 16384
heat_heat_conf_overrides:
  clients_keystone:
    insecure:  True
    endpoint_type: publicURL
{%- raw %}
    auth_uri: "{{ keystone_service_publicurl }}"
{% endraw %}

## Horizon
horizon_wsgi_processes: 1
horizon_wsgi_threads: 1
horizon_wsgi_threads_max: 2

## Ironic
ironic_wsgi_threads: 1
ironic_wsgi_processes_max: 2
ironic_wsgi_processes: 1

{% if swift is defined and swift | bool %}
## Swift
swift_account_server_replicator_workers: 1
swift_server_replicator_workers: 1
swift_object_replicator_workers: 1
swift_account_server_workers: 1
swift_container_server_workers: 1
swift_object_server_workers: 1
swift_proxy_server_workers_max: 2
swift_proxy_server_workers_not_capped: 1
swift_proxy_server_workers_capped: 1
swift_proxy_server_workers: 1
{% endif %}

{% if metering is defined and metering | bool %}
## Ceilometer
ceilometer_notification_workers_max: 2
ceilometer_notification_workers: 1

## AODH
aodh_wsgi_threads: 1
aodh_wsgi_processes_max: 2
aodh_wsgi_processes: 1

## Gnocchi
gnocchi_wsgi_threads: 1
gnocchi_wsgi_processes_max: 2
gnocchi_wsgi_processes: 1
{% endif %}

### Barbican
barbican_wsgi_processes: 2
barbican_wsgi_threads: 1

## Trove
trove_api_workers_max: 2
trove_api_workers: 1
trove_conductor_workers_max: 2
trove_conductor_workers: 1
trove_wsgi_threads: 1
trove_wsgi_processes_max: 2
trove_wsgi_processes: 1

## Sahara
sahara_api_workers_max: 2
sahara_api_workers: 1

# End of low_end_profile
{% endif %}

## Common Glance Overrides
# Set glance_default_store to "swift" if using Cloud Files backend
# or "rbd" if using ceph backend; the latter will trigger ceph to get
# installed on glance. If using a file store, a shared file store is
# recommended. See the OpenStack-Ansible install guide and the OpenStack
# documentation for more details.
# Note that "swift" is automatically set as the default back-end if there
# are any swift hosts in the environment. Use this setting to override
# this automation if you wish for a different default back-end.
# glance_default_store: file

## Ceph pool name for Glance to use
{% if ceph_osd_hosts is defined and ceph_osd_hosts | length > 0 %}
glance_rbd_store_pool: images
glance_rbd_store_chunk_size: 8
glance_ceph_client: glance
glance_default_store: rbd
glance_rbd_store_pool: images
{% endif %}

## Common Neutron Overrides
# Install standards Neutron plus LBaas V2 plugin
neutron_plugin_base:
  - router
  - metering
  - neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2

## Common Nova Overrides
# When nova_libvirt_images_rbd_pool is defined, ceph will be installed on nova
# hosts.
{% if ceph_osd_hosts is defined and ceph_osd_hosts | length > 0 %}
nova_libvirt_images_rbd_pool: vms
{% endif %}

nova_cpu_allocation_ratio: 8.0
nova_nova_conf_overrides:
  DEFAULT:
{%- raw %}
    cpu_allocation_ratio: "{{ nova_cpu_allocation_ratio }}"
{% endraw %}
    block_device_allocate_retries: 60
    block_device_allocate_retries_interval: 5
    block_device_creation_timeout: 300

cinder_cinder_conf_overrides:
  DEFAULT:
    image_volume_cache_enabled: true
{% if ceph_osd_hosts is defined and ceph_osd_hosts | length > 0 %}
  ceph:
    image_volume_cache_enabled: true
{% endif %}

{% if ceph_osd_hosts is defined and ceph_osd_hosts | length > 0 %}
cinder_default_volume_type: rbd
{% else %}
cinder_default_volume_type: lvm
{% endif %}


# If you wish to change the dhcp_domain configured for both nova and neutron
# dhcp_domain: openstacklocal

## Common Glance Overrides when using a Swift back-end
# By default when 'glance_default_store' is set to 'swift' the playbooks will
# expect to use the Swift back-end that is configured in the same inventory.
# If the Swift back-end is not in the same inventory (ie it is already setup
# through some other means) then these settings should be used.
#
# NOTE: Ensure that the auth version matches your authentication endpoint.
#
# NOTE: If the password for glance_swift_store_key contains a dollar sign ($),
# it must be escaped with an additional dollar sign ($$), not a backslash. For
# example, a password of "super$ecure" would need to be entered as
# "super$$ecure" below.  See Launchpad Bug #1259729 for more details.
#
# glance_swift_store_auth_version: 3
# glance_swift_store_auth_address: "https://some.auth.url.com"
# glance_swift_store_user: "OPENSTACK_TENANT_ID:OPENSTACK_USER_NAME"
# glance_swift_store_key: "OPENSTACK_USER_PASSWORD"
# glance_swift_store_container: "NAME_OF_SWIFT_CONTAINER"
# glance_swift_store_region: "NAME_OF_REGION"

## Common Swift Overrides
{% if swift is defined and swift | bool %}
swift_proxy_server_conf_overrides:
  DEFAULT:
    log_statsd_default_sample_rate: 10
{%- raw %}
    log_statsd_metric_prefix: "{{ inventory_hostname }}.swift"
{% endraw %}
    log_statsd_host: localhost
    log_statsd_port: 8125
{% endif %}

## Magnum overrides for cinder integration
{% if magnum is defined and magnum | bool %}
# Currently using a fork where bug fixes relative to the generation of
# AUTH_URL for Keystone V3 is fixed.
magnum_git_repo: "https://github.com/opennext-io/magnum.git"
magnum_git_install_branch: "stable/queens"

magnum_glance_images:
  - name: Fedora-Atomic-28-20180806
    disk_format: qcow2
    image_format: bare
    public: true
    file: Fedora-AtomicHost-28-20180806.0.x86_64.qcow2 
    distro: fedora-atomic
    checksum: "sha256:2b74dec845396df2bfae2e9bda0cfd4e6d8ba185cc30f60be7d5131399b9551f"
magnum_config_overrides:
  cinder:
{%- raw %}
    default_docker_volume_type: "{{ cinder_default_volume_type }}"
{% endraw %}
  drivers:
{%- raw %}
    verify_ca: "{{ magnum_enable_verify_ca | default(True) }}"
    openstack_ca_file: "{{ magnum_ca_file_bundle_path | default('/etc/ssl/certs/openstack_root_ca.pem') }}"
{% endraw %}
{% endif %}

## Common Ceph Overrides
{% if ceph_osd_hosts is defined and ceph_osd_hosts | length > 0 %}
## Custom Ceph Configuration File (ceph.conf)
# By default, your deployment host will connect to one of the mons defined above to
# obtain a copy of your cluster's ceph.conf.  If you prefer, uncomment ceph_conf_file
# and customise to avoid ceph.conf being copied from a mon.
#ceph_conf_file: |
#  [global]
#  fsid = 00000000-1111-2222-3333-444444444444
#  mon_initial_members = mon1.example.local,mon2.example.local,mon3.example.local
#  mon_host = 10.16.5.40,10.16.5.41,10.16.5.42
#  # optionally, you can use this construct to avoid defining this list twice:
{%- raw %}
#  # mon_host = {{ ceph-mon | join(',') }}
{% endraw %}
#  auth_cluster_required = cephx
#  auth_service_required = cephx

## Ceph cluster fsid (must be generated before first run)
## Generate a uuid using: python -c 'import uuid; print(str(uuid.uuid4()))'
generate_fsid: false
fsid: {{ ceph_uuid_seed | default('ceph_uuid') | to_uuid }}

## ceph-ansible settings
## See https://github.com/ceph/ceph-ansible/tree/master/group_vars for
## additional configuration options availble.
{%- raw %}
monitor_address_block: "{{ cidr_networks.container }}"
public_network: "{{ cidr_networks.container }}"
cluster_network: "{{ cidr_networks.storage }}"
{% endraw %}
osd_objectstore: filestore
osd_scenario: lvm
lvm_volumes:
  - data: data
    data_vg: ceph-data
    journal: journal
    journal_vg: ceph-journal
dmcrypt: false
# Set a default 10G journal size (size in MB)
journal_size: {{ ceph_journal_partition_size | default(10240) }} 
# ceph-ansible automatically creates pools & keys for OpenStack services
openstack_config: true
cinder_ceph_client: cinder
{% endif %}

# By default, openstack-ansible configures all OpenStack services to talk to
# RabbitMQ over encrypted connections on port 5671. To opt-out of this default,
# set the rabbitmq_use_ssl variable to 'false'. The default setting of 'true'
# is highly recommended for securing the contents of RabbitMQ messages.
# rabbitmq_use_ssl: false

# RabbitMQ management plugin is enabled by default, the guest user has been
# removed for security reasons and a new userid 'monitoring' has been created
# with the 'monitoring' user tag. In order to modify the userid, uncomment the
# following and change 'monitoring' to your userid of choice.
rabbitmq_monitoring_userid: monitoring

## HAProxy and keepalived
# All the previous variables are used inside a var, in the group vars.
# You can override the current keepalived definition (see
# group_vars/all/keepalived.yml) in your user space if necessary.
#
# Uncomment this to disable keepalived installation (cf. documentation)
# haproxy_use_keepalived: False
#
# HAProxy Keepalived configuration (cf. documentation)
# Make sure that this is set correctly according to the CIDR used for your
# internal and external addresses.

## Load Balancer Configuration (haproxy/keepalived)
{% if is_aio is undefined or (is_aio is defined and not is_aio) %}
haproxy_keepalived_external_vip_cidr: "{{ hapk_ext_vip_cidr | default('172.31.0.100/32') }}"
haproxy_keepalived_internal_vip_cidr: "{{ hapk_int_vip_cidr | default('172.29.236.100/22') }}"
haproxy_keepalived_external_interface: {{ hapk_ext_itf | default('eno1') }}
haproxy_keepalived_internal_interface: {{ hapk_int_itf | default('br-mgmt') }}
{% endif %}

# Defines the default VRRP id used for keepalived with haproxy.
# Overwrite it to your value to make sure you don't overlap
# with existing VRRPs id on your network. Default is 10 for the external and 11 for the
# internal VRRPs
# haproxy_keepalived_external_virtual_router_id:
# haproxy_keepalived_internal_virtual_router_id:

# Defines the VRRP master/backup priority. Defaults respectively to 100 and 20
# haproxy_keepalived_priority_master:
# haproxy_keepalived_priority_backup:

haproxy_stats_enabled: True
haproxy_stats_bind_address: 127.0.0.1
haproxy_stats_port: 1936
haproxy_username: admin
{% if proxy_host is defined and proxy_host | length > 0 %}
# Needed to pass healthcheck-infrastructure.yml
keepalived_ping_address: "{{ proxy_host }}"
{% endif %}

## HAProxy SSL certificates, keys, and CA certificates from your own trusted certificate authority.
{% if haproxy_ssl is defined and haproxy_ssl | bool %}
haproxy_user_ssl_cert: {{ haproxy_ssl_cert_path | default('/etc/openstack_deploy/ssl/openstack_cert.crt') }}
haproxy_user_ssl_key: {{ haproxy_ssl_key_path | default('/etc/openstack_deploy/ssl/openstack_key.key') }}
haproxy_user_ssl_ca_cert: {{ haproxy_ssl_ca_cert_path | default('/etc/openstack_deploy/ssl/openstack_ca.pem') }}
{% endif %}

## HAProxy load-balancing extra congig for the monitoring and logging services
haproxy_extra_services:
  - service:
    haproxy_service_name: influxdb_admin
    haproxy_ssl: False
{%- raw %}
    haproxy_backend_nodes: "{{ groups['influxdb'] | default([]) }}"
    haproxy_port: "{{ influxdb_port }}"
{% endraw %}
    haproxy_balance_type: tcp
  - service:
    haproxy_service_name: influxdb
    haproxy_ssl: False
{%- raw %}
    haproxy_backend_nodes: "{{ groups['influxdb'] | default([]) }}"
    haproxy_port: "{{ influxdb_port }}"
    haproxy_backend_port: "{{ influxdb_port }}"
{% endraw %}
    haproxy_balance_type: http
    haproxy_backend_options:
      - "httpchk HEAD /ping"
    haproxy_acls:
      read_queries:
        rule: "path_sub -i query"
      write_queries:
        rule: "path_sub -i write"
        backend_name: "influxdb_relay"
  - service:
    haproxy_service_name: influxdb_relay
    haproxy_ssl: False
{%- raw %}    
    haproxy_backend_nodes: "{{ groups['influxdb_relay'] | default([]) }}"
    haproxy_port: "{{ influxdb_relay_port }}"
    haproxy_backend_port: "{{ influxdb_relay_port }}"
{% endraw %}
    haproxy_balance_type: http
    haproxy_balance_alg: roundrobin
    haproxy_acls:
      write_queries:
        rule: "path_sub -i write"
      read_queries:
        rule: "path_sub -i query"
        backend_name: "influxdb"
  - service:
    haproxy_service_name: grafana
{%- raw %}    
    haproxy_backend_nodes: "{{ groups['grafana'] | default([]) }}"
    haproxy_ssl: "{{ haproxy_ssl }}"
    haproxy_port: "{{ grafana_port }}"
{% endraw %}    
    haproxy_balance_type: http
    haproxy_balance_alg: source
    haproxy_backend_options:
      - "httpchk GET /"
    haproxy_backend_reqrep: '^([^\ ]*\ /)grafana[/]?(.*) \1\2'
  - service:
    haproxy_service_name: redis
    haproxy_ssl: False
{%- raw %}
    haproxy_backend_nodes: "{{ groups['redis'] | default([]) }}"
    haproxy_port: "{{ redis_port }}"
    haproxy_backend_port: "{{ redis_port }}"
{% endraw %}
    haproxy_balance_type: tcp
    haproxy_balance_alg: roundrobin
    haproxy_timeout_server: "6s"
    haproxy_backend_options:
      - "tcp-check"
    haproxy_backend_arguments:
      - "tcp-check connect"
      - "tcp-check send PING\r\n"
      - "tcp-check expect string +PONG"
      - "tcp-check send QUIT\r\n"
      - "tcp-check expect string +OK"
# No need anymore for Elasticsearch load-balacing for two reasons
# 1 - Kibana is connected to a local Elasticsearch clients_keystone
# 2 - Fluentd is co-located with the Elasticsearch servers in the cluster
#  - service:
#    haproxy_service_name: elasticsearch
#{%- raw %}
#    haproxy_backend_nodes: "{{ groups['elasticsearch'] | default([]) }}"
#    haproxy_ssl: "{{ haproxy_ssl }}"
#    haproxy_port: "{{ elastic_port }}"
#    haproxy_backend_port: "{{ elastic_port }}"
#{% endraw %}    
#    haproxy_balance_type: tcp
#    haproxy_backend_options:
#      - "tcpka"
#      - "httpchk GET /_cat/health"
  - service:
    haproxy_service_name: kibana
{%- raw %}    
    haproxy_backend_nodes: "{{ groups['kibana'] | default([]) }}"
    haproxy_ssl: "{{ haproxy_ssl }}"
    haproxy_port: "{{ kibana_server_port }}"
    haproxy_backend_port: "{{ kibana_server_port }}"
{% endraw %}
    haproxy_balance_type: http
    haproxy_balance_alg: source
    haproxy_backend_options:
      - "httpchk GET /"
    haproxy_backend_reqrep: '^([^\ :]*)\ /kibana/(.*) \1\ /\2'
    haproxy_acls:
      acl_kibana:
        rule: "path_beg /kibana"
        backend_name: "kibana"
  - service:
      haproxy_service_name: fluentd
      haproxy_ssl: False
{%- raw %}
      haproxy_backend_nodes: "{{ groups['fluentd'] | default([]) }}"
      haproxy_port: "{{ tdagent_syslog_port }}"
{% endraw %}
      haproxy_balance_type: tcp
      haproxy_balance_alg: roundrobin
      haproxy_backend_options:
        - "tcpka"
  - service:
    haproxy_service_name: uchiwa
{%- raw %}    
    haproxy_backend_nodes: "{{ groups['sensu'] | default([]) }}"
    haproxy_ssl: "{{ haproxy_ssl }}"
    haproxy_port: "{{ uchiwa_port }}"
    haproxy_backend_port: "{{ uchiwa_port }}"
{% endraw %}
    haproxy_balance_type: http
    haproxy_balance_alg: source
    haproxy_backend_options:
      - "httpchk GET /"
  - service:
      haproxy_service_name: sensu_api
      haproxy_ssl: False
{%- raw %}
      haproxy_backend_nodes: "{{ groups['sensu'] | default([]) }}"
      haproxy_port: "{{ sensu_api_port }}"
{% endraw %}
      haproxy_balance_type: tcp
      haproxy_backend_options:
        - "httpchk GET /"

## Tempest settings
tempest_public_subnet_cidr: {{ tempest_subnet | default('172.29.248.0/22') }}
tempest_public_subnet_allocation_pools: "{{ tempest_pool | default('172.29.249.110-172.29.249.200') }}"
tempest_install: {{ run_tempest_tests | default('yes') }}
tempest_run: {{ run_tempest_tests | default('yes') }}

## Additional pinning generator that will allow for more packages to be pinned as you see fit.
## All pins allow for package and versions to be defined. Be careful using this as versions
## are always subject to change and updates regarding security will become your problem from this
## point on. Pinning can be done based on a package version, release, or origin. Use "*" in the
## package name to indicate that you want to pin all package to a particular constraint.
# apt_pinned_packages:
#   - { package: "lxc", version: "1.0.7-0ubuntu0.1" }
#   - { package: "libvirt-bin", version: "1.2.2-0ubuntu13.1.9" }
#   - { package: "rabbitmq-server", origin: "www.rabbitmq.com" }
#   - { package: "*", release: "MariaDB" }

## Environment variable settings
# This allows users to specify the additional environment variables to be set
# which is useful in setting where you working behind a proxy. If working behind
# a proxy It's important to always specify the scheme as "http://". This is what
# the underlying python libraries will handle best. This proxy information will be
# placed both on the hosts and inside the containers.

## Example environment variable setup:
## This is used by apt-cacher-ng to download apt packages:
# proxy_env_url: http://username:pa$$w0rd@10.10.10.9:9000/

{%- raw %}
## (1) This sets up a permanent environment, used during and after deployment:
# no_proxy_env: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
# global_environment_variables:
#   HTTP_PROXY: "{{ proxy_env_url }}"
#   HTTPS_PROXY: "{{ proxy_env_url }}"
#   NO_PROXY: "{{ no_proxy_env }}"
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "{{ no_proxy_env }}"
#
## (2) This is applied only during deployment, nothing is left after deployment is complete:
# deployment_environment_variables:
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['keystone_all'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
{% endraw %}

## SSH connection wait time
# If an increased delay for the ssh connection check is desired,
# uncomment this variable and set it appropriately.
#ssh_delay: 5
